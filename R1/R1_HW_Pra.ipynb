{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOPXxjexDMDZ"
      },
      "source": [
        "# baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hvn98KdEE9_"
      },
      "source": [
        "將`關鍵字`比對換成`向量相似度`比對。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q langgraph langchain langchain-community langchain-huggingface transformers bitsandbytes chromadb accelerate"
      ],
      "metadata": {
        "id": "8XsIB2JxOLjl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmKk76tcUB09",
        "outputId": "1652c942-afd3-494a-b256-f98ab5524f0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV4-rfIDDz7D"
      },
      "source": [
        "> 請將目前使用關鍵字比對的 route_by_query，改為使用向量相似度進行分類，並設一個合理的相似度門檻，根據檢索結果的分數判斷是否走 RAG 流程。  \n",
        "例如用向量相似度及自訂 threshold 決定要不要分到 retriever。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2jMeAOuJ2OM"
      },
      "source": [
        "*斜體文字*> Hint：similarity_search_with_score(...)  \n",
        "可參考去年的讀書會 R4：向量資料庫的基本操作"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uGc7o1k0LvdY"
      },
      "outputs": [],
      "source": [
        "docs_text = \"\"\"\n",
        "火影代數\t姓名\t師傅\t徒弟\n",
        "初代\t千手柱間\t無明確記載\t猿飛日斬、水戶門炎、轉寢小春\n",
        "二代\t千手扉間\t千手柱間（兄長）\t猿飛日斬、志村團藏、宇智波鏡等\n",
        "三代\t猿飛日斬\t千手柱間、千手扉間\t自來也、大蛇丸、千手綱手（傳說三忍）\n",
        "四代\t波風湊\t自來也\t旗木卡卡西、宇智波帶土、野原琳\n",
        "五代\t千手綱手\t猿飛日斬\t春野櫻、志乃等（主要為春野櫻）\n",
        "六代\t旗木卡卡西\t波風湊\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\n",
        "七代\t漩渦鳴人\t自來也、旗木卡卡西\t木葉丸等（主要為木葉丸）\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 模型載入與生成器定義 ---\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "model_id = \"MediaTek-Research/Breeze-7B-Instruct-v1_0\"\n",
        "\n",
        "# 量化設定（使用 4-bit 載入）\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_threshold=6.0,\n",
        ")\n",
        "\n",
        "# 載入 tokenizer 和模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=quant_config,\n",
        ")\n",
        "\n",
        "# HuggingFace 的生成管線（可用於 pipeline 生成）\n",
        "generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        "    return_full_text=False  # 僅回傳模型生成的內容\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "99b24f06c3bb4bef8960fd555405c468",
            "b70fae35c63f4092a6f33611d85a81f3",
            "a2e13ac9bc354d77a03e2f9e6f82df11",
            "ce2d0c8ede6a4d678c0185fe26062264",
            "1d96051b87a84f92be51396b73c9ef6c",
            "d140a8ae88d4439e9573468da99cd320",
            "ae1e81330ef64a5a840c1bf23eb39045",
            "2b9ceb4eeaca4e3fb6551a0e4ecf2040",
            "18838f1ca45d48ecb84c022122abe077",
            "1c9fbfa1f1664fd994b6b4cd6b1851b2",
            "6f5e6ad5de794c938c7a25de808c71eb"
          ]
        },
        "id": "yq2U-JBqTIya",
        "outputId": "69e3b527-3b87-4b7f-8f71-9dd23b20b5c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99b24f06c3bb4bef8960fd555405c468"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "EvJxczMsfH_T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 改寫重點：用向量相似度決定是否執行 RAG\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# shutil.rmtree(\"document_store\", ignore_errors=True)\n",
        "\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-large\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# 包裝成 Document 時加提示詞\n",
        "docs = [Document(page_content=f\"passage: {txt.strip()}\") for txt in docs_text.strip().split(\"\\n\\n\")]\n",
        "\n",
        "persist_path = \"document_store\"\n",
        "collection_name = \"naruto_collection\"\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding_model,\n",
        "    collection_name=collection_name\n",
        ")\n",
        "\n",
        "from typing_extensions import TypedDict, List\n",
        "\n",
        "class RAGState(TypedDict):\n",
        "    query: str\n",
        "    docs: List[Document]\n",
        "    answer: str\n",
        "    score: float\n",
        "\n",
        "\n",
        "# def retrieve_node(state: RAGState) -> RAGState:\n",
        "#     query = state[\"query\"]\n",
        "#     results = vectorstore.similarity_search_with_relevance_scores(query, k=3)\n",
        "#     docs = [doc for doc, score in results]\n",
        "#     max_score = max([score for _, score in results], default=0.0)\n",
        "#     return {\"query\": query, \"docs\": docs, \"answer\": \"\", \"score\": max_score}\n",
        "\n",
        "def retrieve_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"query\"]\n",
        "    results = vectorstore.similarity_search_with_score(query, k=3)\n",
        "\n",
        "    docs = [doc for doc, score in results]\n",
        "    max_score = max([1 - score for _, score in results], default=0.0)\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"docs\": docs,\n",
        "        \"answer\": \"\",\n",
        "        \"score\": max_score  # 這裡就是相似度語意\n",
        "    }\n",
        "\n",
        "def route_by_similarity(state: RAGState) -> str:\n",
        "    threshold = 0.50  # 可依 embedding 調整\n",
        "    if state[\"score\"] >= threshold:\n",
        "        return \"rag\"\n",
        "    else:\n",
        "        return \"direct\"\n",
        "\n",
        "def generate_node(state: RAGState) -> RAGState:\n",
        "    query, docs = state[\"query\"], state[\"docs\"]\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "    prompt = (\n",
        "        f\"你是一個知識型助手，請根據以下內容回答問題：\\n\\n\"\n",
        "        f\"內容：{context}\\n\\n\"\n",
        "        f\"問題：{query}\\n\\n回答：\"\n",
        "    )\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": output, \"score\": state[\"score\"]}\n",
        "\n",
        "\n",
        "def direct_generate_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"query\"]\n",
        "    prompt = f\"請回答以下問題：{query}\\n\\n回答：\"\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": [], \"answer\": output, \"score\": state[\"score\"]}\n",
        "\n",
        "\n",
        "def route_by_similarity(state: RAGState) -> str:\n",
        "    threshold = 0.50\n",
        "    if state[\"score\"] >= threshold:\n",
        "        print(f\"相似度 {state['score']:.2f} → 使用 RAG\")\n",
        "        return \"rag\"\n",
        "    else:\n",
        "        print(f\"相似度 {state['score']:.2f} → 使用直接生成\")\n",
        "        return \"direct\"\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "graph_builder = StateGraph(RAGState)\n",
        "\n",
        "graph_builder.set_entry_point(\"retriever\")\n",
        "graph_builder.add_node(\"retriever\", RunnableLambda(retrieve_node))\n",
        "graph_builder.add_node(\"generator\", RunnableLambda(generate_node))\n",
        "graph_builder.add_node(\"direct_generator\", RunnableLambda(direct_generate_node))\n",
        "\n",
        "# 分流條件改為根據相似度分流\n",
        "graph_builder.add_conditional_edges(\n",
        "    source=\"retriever\",\n",
        "    path=RunnableLambda(route_by_similarity),\n",
        "    path_map={\n",
        "        \"rag\": \"generator\",\n",
        "        \"direct\": \"direct_generator\",\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_builder.add_edge(\"generator\", END)\n",
        "graph_builder.add_edge(\"direct_generator\", END)\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# 使用時初始 state 要多加 score 欄位：\n",
        "# init_state = {\"query\": \"火影是誰？\", \"docs\": [], \"answer\": \"\", \"score\": 0.0}\n"
      ],
      "metadata": {
        "id": "fIaJeRNmHOSx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"使用者: \")\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"掰啦！\")\n",
        "        break\n",
        "\n",
        "    # 初始 State，score 記得給 0，retriever 會補上\n",
        "    init_state: RAGState = {\n",
        "        \"query\": user_input,\n",
        "        \"docs\": [],\n",
        "        \"answer\": \"\",\n",
        "        \"score\": 0.0\n",
        "    }\n",
        "\n",
        "    # 呼叫 LangGraph\n",
        "    result = graph.invoke(init_state)\n",
        "    raw_output = result[\"answer\"]\n",
        "\n",
        "    # 處理輸出格式\n",
        "    answer_text = raw_output.split(\"回答：\")[-1].strip()\n",
        "    print(\"回答：\", answer_text)\n",
        "    print(\"===\" * 20, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y9I2sMFLOqc",
        "outputId": "cb06ca45-bebe-44a7-8267-9a108361141f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "開始對話吧（輸入 q 結束）\n",
            "使用者: 誰是第四代火影?\n",
            "相似度 0.68 → 使用 RAG\n",
            "回答： 第四代火影是波風湊。\n",
            "============================================================ \n",
            "\n",
            "使用者: 第四代火影的師傅是誰?\n",
            "相似度 0.70 → 使用 RAG\n",
            "回答： 第四代火影的師傅是自來也。\n",
            "============================================================ \n",
            "\n",
            "使用者: 第四代火影的徒弟有哪些人?\n",
            "相似度 0.73 → 使用 RAG\n",
            "回答： 第四代火影的徒弟有旗木卡卡西、宇智波帶土、野原琳。\n",
            "============================================================ \n",
            "\n",
            "使用者: 相對論是誰發明的?\n",
            "相似度 0.52 → 使用直接生成\n",
            "回答： 相對論是由愛因斯坦（Albert Einstein）在 1905 年提出的。\n",
            "============================================================ \n",
            "\n",
            "使用者: q\n",
            "掰啦！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wflAEri34xe5",
        "outputId": "b480e3cf-9c0d-4c68-fd90-fcb54d309129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "開始對話吧（輸入 q 結束）\n",
            "使用者: 誰是第四代火影?\n",
            "route: cosine_sim = 0.8227\n",
            "跑到 → naruto\n",
            "回答： 第四代火影是波風湊。\n",
            "============================================================ \n",
            "\n",
            "使用者: 第四代火影的師傅是誰?\n",
            "route: cosine_sim = 0.8646\n",
            "跑到 → naruto\n",
            "回答： 第四代火影的師傅是自來也。\n",
            "============================================================ \n",
            "\n",
            "使用者: 第四代火影的徒弟有哪些人?\n",
            "route: cosine_sim = 0.8726\n",
            "跑到 → naruto\n",
            "回答： 第四代火影波風湊的徒弟包括旗木卡卡西、宇智波帶土、野原琳。\n",
            "============================================================ \n",
            "\n",
            "使用者: 相對論是誰發明的?\n",
            "route: cosine_sim = 0.1666\n",
            "跑到 → general\n",
            "回答： 相對論是由愛因斯坦（Albert Einstein）在 1905 年提出的。\n",
            "============================================================ \n",
            "\n",
            "使用者: q\n",
            "掰啦！\n"
          ]
        }
      ],
      "source": [
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"使用者: \")\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"掰啦！\")\n",
        "        break\n",
        "\n",
        "    init_state: RAGState = {\n",
        "        \"query\": user_input,\n",
        "        \"docs\": [],\n",
        "        \"answer\": \"\"\n",
        "    }\n",
        "\n",
        "    result = graph.invoke(init_state)\n",
        "    raw_output = result[\"answer\"]\n",
        "\n",
        "    answer_text = raw_output.split(\"回答：\")[-1].strip()\n",
        "    print(\"回答：\", answer_text)\n",
        "    print(\"===\" * 20, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8blDsDnDpbO"
      },
      "source": [
        "# advance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofkLZjzHENNT"
      },
      "source": [
        "改成能支援多輪問答（Multi-turn RAG），並能根據前面的query判斷問題。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2VXz7FxEONI"
      },
      "source": [
        "> 請將 RAGState 加入 history 欄位，並在生成回答時，將歷史對話與當前問題一起組成 prompt。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eSvoKFiKqP5"
      },
      "source": [
        "> Hint：\n",
        "```\n",
        "class MultiTurnRAGState(TypedDict):  \n",
        "    history: List[str]  \n",
        "    query: str  \n",
        "    docs: List[Document]  \n",
        "    answer: str\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "50GrH76YP1eu"
      },
      "outputs": [],
      "source": [
        "docs_text = \"\"\"\n",
        "火影代數\t姓名\t師傅\t徒弟\n",
        "初代\t千手柱間\t無明確記載\t猿飛日斬、水戶門炎、轉寢小春\n",
        "二代\t千手扉間\t千手柱間（兄長）\t猿飛日斬、志村團藏、宇智波鏡等\n",
        "三代\t猿飛日斬\t千手柱間、千手扉間\t自來也、大蛇丸、千手綱手（傳說三忍）\n",
        "四代\t波風湊\t自來也\t旗木卡卡西、宇智波帶土、野原琳\n",
        "五代\t千手綱手\t猿飛日斬\t春野櫻、志乃等（主要為春野櫻）\n",
        "六代\t旗木卡卡西\t波風湊\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\n",
        "七代\t漩渦鳴人\t自來也、旗木卡卡西\t木葉丸等（主要為木葉丸）\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "A4QWyJxhP5Ir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63f54da-c206-456f-c4a6-d17cab6caade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "開始對話吧（輸入 q 結束）\n",
            "使用者: 第四代火影是誰?\n",
            "AI 助理: 第四代火影是波風湊。\n",
            "使用者: 他的師父是誰?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI 助理: 他的師父是張三豐。\n",
            "使用者: q\n",
            "結束對話\n"
          ]
        }
      ],
      "source": [
        "# ✅ 多輪問答版本：Multi-turn RAG 支援\n",
        "\n",
        "from typing_extensions import TypedDict, List\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# ✅ 定義多輪狀態\n",
        "class MultiTurnRAGState(TypedDict):\n",
        "    history: List[str]\n",
        "    query: str\n",
        "    docs: List[Document]\n",
        "    answer: str\n",
        "\n",
        "# ✅ 更新 retriever node，根據上下文判斷查詢內容\n",
        "\n",
        "def retrieve_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
        "    # 可選：加入歷史上下文到查詢中\n",
        "    full_query = \"\\n\".join(state[\"history\"][-2:] + [state[\"query\"]])\n",
        "    full_query = f\"query: {full_query}\"\n",
        "\n",
        "    results = vectorstore.similarity_search_with_relevance_scores(full_query, k=3)\n",
        "    docs = [doc for doc, _ in results]\n",
        "\n",
        "    return {\n",
        "        \"history\": state[\"history\"],\n",
        "        \"query\": state[\"query\"],\n",
        "        \"docs\": docs,\n",
        "        \"answer\": \"\"\n",
        "    }\n",
        "\n",
        "# ✅ generator node，加入對應 context\n",
        "\n",
        "def generate_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
        "    query, docs = state[\"query\"], state[\"docs\"]\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    prompt = (\n",
        "        f\"你是一位知識型助手，請根據以下內容回答問題：\\n\\n\"\n",
        "        f\"內容：{context}\\n\\n\"\n",
        "        f\"問題：{query}\\n\\n回答：\"\n",
        "    )\n",
        "    output = generator(prompt, max_new_tokens=300)[0][\"generated_text\"]\n",
        "\n",
        "    return {\n",
        "        \"history\": state[\"history\"] + [query, output],\n",
        "        \"query\": query,\n",
        "        \"docs\": docs,\n",
        "        \"answer\": output\n",
        "    }\n",
        "\n",
        "# ✅ direct answer node（不檢索）\n",
        "def direct_generate_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
        "    query = state[\"query\"]\n",
        "    prompt = f\"請回答以下問題：{query}\\n\\n回答：\"\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "\n",
        "    return {\n",
        "        \"history\": state[\"history\"] + [query, output],\n",
        "        \"query\": query,\n",
        "        \"docs\": [],\n",
        "        \"answer\": output\n",
        "    }\n",
        "\n",
        "# ✅ 條件分流：根據 query 內容決定是否走 RAG\n",
        "\n",
        "def route_by_similarity(state: MultiTurnRAGState) -> str:\n",
        "    keywords = [\"火影\", \"歷代\", \"忍者\"]\n",
        "    if any(k in state[\"query\"] for k in keywords):\n",
        "        return \"rag\"\n",
        "    else:\n",
        "        return \"direct\"\n",
        "\n",
        "# ✅ 建立 graph\n",
        "rag_graph = StateGraph(MultiTurnRAGState)\n",
        "rag_graph.set_entry_point(\"router\")\n",
        "\n",
        "rag_graph.add_node(\"router\", RunnableLambda(lambda x: x))\n",
        "rag_graph.add_node(\"retriever\", RunnableLambda(retrieve_node))\n",
        "rag_graph.add_node(\"generator\", RunnableLambda(generate_node))\n",
        "rag_graph.add_node(\"direct_generator\", RunnableLambda(direct_generate_node))\n",
        "\n",
        "rag_graph.add_conditional_edges(\n",
        "    source=\"router\",\n",
        "    path=RunnableLambda(route_by_similarity),\n",
        "    path_map={\n",
        "        \"rag\": \"retriever\",\n",
        "        \"direct\": \"direct_generator\"\n",
        "    }\n",
        ")\n",
        "\n",
        "rag_graph.add_edge(\"retriever\", \"generator\")\n",
        "rag_graph.add_edge(\"generator\", END)\n",
        "rag_graph.add_edge(\"direct_generator\", END)\n",
        "\n",
        "rag_executor = rag_graph.compile()\n",
        "\n",
        "# ✅ 啟動對話\n",
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "\n",
        "state = {\"history\": [], \"query\": \"\", \"docs\": [], \"answer\": \"\"}\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"使用者: \")\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"結束對話\")\n",
        "        break\n",
        "\n",
        "    state[\"query\"] = user_input\n",
        "    result = rag_executor.invoke(state)\n",
        "    answer = result[\"answer\"]\n",
        "    print(\"AI 助理:\", answer.strip())\n",
        "\n",
        "    # 將回答更新到狀態中，準備下輪\n",
        "    state = result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Jrna_evTCC",
        "outputId": "fe0bc544-ba23-4b77-eca8-ae38c698bd83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "開始對話吧（輸入 q 結束）\n",
            "使用者: 第四代火影是誰?\n",
            "route: cosine_sim = 0.8092\n",
            "跑到 → retriever\n",
            "retrieve combined query: '第四代火影是誰?'\n",
            "AI 助理: 第四代火影是波風湊。\n",
            "==================================================================================================================================================================================== \n",
            "\n",
            "使用者: 他的師父是誰?\n",
            "route: cosine_sim = 0.5529\n",
            "跑到 → retriever\n",
            "retrieve combined query: '第四代火影是誰?\\n他的師父是誰?'\n",
            "AI 助理: 第四代火影的師父是自來也。\n",
            "==================================================================================================================================================================================== \n",
            "\n",
            "使用者: 他的徒弟有哪些人?\n",
            "route: cosine_sim = 0.6542\n",
            "跑到 → retriever\n",
            "retrieve combined query: '第四代火影是誰?\\n他的師父是誰?\\n他的徒弟有哪些人?'\n",
            "AI 助理: 他的徒弟有以下人：旗木卡卡西、宇智波帶土、野原琳。\n",
            "==================================================================================================================================================================================== \n",
            "\n",
            "使用者: 相對論是他發明的嗎?\n",
            "route: cosine_sim = 0.1118\n",
            "跑到 → general\n",
            "AI 助理: 相對論不是第四代火影所發明的。相對論是物理學家阿爾伯特·愛因斯坦在1905年提出的，他提出了廣義相對論，後人又提出狹義相對論。相對論主要是研究加速度和重力的關係，以及加速度和時間、空間的關係。\n",
            "==================================================================================================================================================================================== \n",
            "\n",
            "使用者: q\n",
            "掰啦！\n"
          ]
        }
      ],
      "source": [
        "global_history: List[str] = []\n",
        "\n",
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "while True:\n",
        "    user_input = input(\"使用者: \")\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"掰啦！\")\n",
        "        break\n",
        "\n",
        "    state = {\"history\": global_history, \"query\": user_input}\n",
        "    result = graph.invoke(state)\n",
        "\n",
        "    answer = result[\"answer\"].split(\"回答：\")[-1].strip()\n",
        "    print(\"AI 助理:\", answer)\n",
        "    print(\"===\" * 60, \"\\n\")\n",
        "\n",
        "    global_history = result[\"history\"]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99b24f06c3bb4bef8960fd555405c468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b70fae35c63f4092a6f33611d85a81f3",
              "IPY_MODEL_a2e13ac9bc354d77a03e2f9e6f82df11",
              "IPY_MODEL_ce2d0c8ede6a4d678c0185fe26062264"
            ],
            "layout": "IPY_MODEL_1d96051b87a84f92be51396b73c9ef6c"
          }
        },
        "b70fae35c63f4092a6f33611d85a81f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d140a8ae88d4439e9573468da99cd320",
            "placeholder": "​",
            "style": "IPY_MODEL_ae1e81330ef64a5a840c1bf23eb39045",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a2e13ac9bc354d77a03e2f9e6f82df11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b9ceb4eeaca4e3fb6551a0e4ecf2040",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18838f1ca45d48ecb84c022122abe077",
            "value": 4
          }
        },
        "ce2d0c8ede6a4d678c0185fe26062264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c9fbfa1f1664fd994b6b4cd6b1851b2",
            "placeholder": "​",
            "style": "IPY_MODEL_6f5e6ad5de794c938c7a25de808c71eb",
            "value": " 4/4 [01:54&lt;00:00, 23.65s/it]"
          }
        },
        "1d96051b87a84f92be51396b73c9ef6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d140a8ae88d4439e9573468da99cd320": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae1e81330ef64a5a840c1bf23eb39045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b9ceb4eeaca4e3fb6551a0e4ecf2040": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18838f1ca45d48ecb84c022122abe077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c9fbfa1f1664fd994b6b4cd6b1851b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f5e6ad5de794c938c7a25de808c71eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}